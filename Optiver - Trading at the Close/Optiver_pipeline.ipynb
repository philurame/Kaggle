{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from typing import Any\n",
    "from utils import *\n",
    "class SeedAll:\n",
    "  def __init__(self, seed=42): self.seed = seed\n",
    "  def __call__(self):\n",
    "    random.seed(self.seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(self.seed)\n",
    "    np.random.seed(self.seed)\n",
    "    torch.manual_seed(self.seed)\n",
    "    torch.cuda.manual_seed(self.seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "seedAll = SeedAll(42)\n",
    "seedAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5237980, 17)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_df = \"optiver-trading-at-the-close/train.csv\"\n",
    "df = reduce_mem_usage(pd.read_csv(path_df).reset_index(drop=True, inplace=True))\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESS IDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit \n",
    "\n",
    "def pdfCauchy(x, A, B): # positive 0-shifted caushy distribution\n",
    "  return 1/(A + (B*x)**2)\n",
    "\n",
    "def preprocess(df):\n",
    "  seedAll() \n",
    "  df = df.query('not (bid_price.isnull() and ask_price.isnull() and wap.isnull())')\n",
    "\n",
    "  df.far_price = df.far_price.clip(0.5,1.5)\n",
    "  ys, xs = np.histogram(abs(df.far_price - df.far_price.mean()), bins=1000)\n",
    "  far_cauchy_params, _ = curve_fit(pdfCauchy, xs[:-1], ys) \n",
    "  df.loc[df.far_price.isna(), 'far_price'] = \\\n",
    "    random.choices(\n",
    "      xs:=np.arange(-1.3, 1.3, step=0.00001), \n",
    "      weights = pdfCauchy(xs, *near_cauchy_params), \n",
    "      k = df.far_price.isna().sum()\n",
    "    )\n",
    "  \n",
    "  ys, xs = np.histogram(abs(df.near_price - df.near_price.mean()), bins=1000)\n",
    "  near_cauchy_params, _ = curve_fit(pdfCauchy, xs[:-1], ys) \n",
    "  df.loc[df.near_price.isna(), 'near_price'] = \\\n",
    "    random.choices(\n",
    "      xs:=np.arange(-1.3, 1.3, step=0.00001), \n",
    "      weights = pdfCauchy(xs, *near_cauchy_params), \n",
    "      k = df.near_price.isna().sum()\n",
    "    )\n",
    "\n",
    "  df['sign_imbalance_size'] = df.imbalance_size * df.imbalance_buy_sell_flag\n",
    "  df.drop(columns=[\"imbalance_buy_sell_flag\"], inplace=True)\n",
    "  return df, far_cauchy_params, near_cauchy_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMBALANCES IDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import njit, prange\n",
    "from itertools import combinations\n",
    "\n",
    "@njit(parallel=True)\n",
    "def compute_triplet_imbalance(df_values, comb_indices):\n",
    "  '''\n",
    "  a<b<c -> (c-b)/(b-a)\n",
    "  returns np.array with shape (df_raws, len(comb_cols))\n",
    "  '''\n",
    "  num_rows = df_values.shape[0]\n",
    "  num_combinations = len(comb_indices)\n",
    "  imbalance_features = np.empty((num_rows, num_combinations))\n",
    "  for i in prange(num_combinations):\n",
    "    a, b, c = comb_indices[i]\n",
    "    for j in range(num_rows):\n",
    "      max_val = max(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "      min_val = min(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "      mid_val = df_values[j, a] + df_values[j, b] + df_values[j, c] - min_val - max_val\n",
    "      if mid_val == min_val:\n",
    "        imbalance_features[j, i] = np.nan\n",
    "      else:\n",
    "        imbalance_features[j, i] = (max_val - mid_val) / (mid_val - min_val)\n",
    "  return imbalance_features\n",
    "\n",
    "def calculate_triplet_imbalance_numba(px_cols, df):\n",
    "  '''\n",
    "  price_cols -> combs_3(price_cols) -> triplet_imbalance\n",
    "  returns dataframe with cols=new_features\n",
    "  '''\n",
    "  df_values = df[px_cols].values\n",
    "  comb_indices = [(px_cols.index(a), px_cols.index(b), px_cols.index(c)) for a, b, c in combinations(px_cols, 3)]\n",
    "  features_array = compute_triplet_imbalance(df_values, comb_indices)\n",
    "  columns  = [f\"{a}_{b}_{c}_imb\" for a, b, c in combinations(px_cols, 3)]\n",
    "  features = pd.DataFrame(features_array, columns=columns)\n",
    "  return features\n",
    "\n",
    "@njit(parallel=True)\n",
    "def calculate_last_imbalance_change(sgn_values):\n",
    "  feature = np.zeros(sgn_values.shape[0])\n",
    "  for i in prange(1,sgn_values.shape[0]):\n",
    "    if sgn_values[i] == sgn_values[i-1]:\n",
    "      feature[i] = feature[i-1]+1\n",
    "    else:\n",
    "      feature[i] = 0\n",
    "  return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_WEIGHTS = [\n",
    "    0.004, 0.001, 0.002, 0.006, 0.004, 0.004, 0.002, 0.006, 0.006, 0.002, 0.002, 0.008,\n",
    "    0.006, 0.002, 0.008, 0.006, 0.002, 0.006, 0.004, 0.002, 0.004, 0.001, 0.006, 0.004,\n",
    "    0.002, 0.002, 0.004, 0.002, 0.004, 0.004, 0.001, 0.001, 0.002, 0.002, 0.006, 0.004,\n",
    "    0.004, 0.004, 0.006, 0.002, 0.002, 0.04 , 0.002, 0.002, 0.004, 0.04 , 0.002, 0.001,\n",
    "    0.006, 0.004, 0.004, 0.006, 0.001, 0.004, 0.004, 0.002, 0.006, 0.004, 0.006, 0.004,\n",
    "    0.006, 0.004, 0.002, 0.001, 0.002, 0.004, 0.002, 0.008, 0.004, 0.004, 0.002, 0.004,\n",
    "    0.006, 0.002, 0.004, 0.004, 0.002, 0.004, 0.004, 0.004, 0.001, 0.002, 0.002, 0.008,\n",
    "    0.02 , 0.004, 0.006, 0.002, 0.02 , 0.002, 0.002, 0.006, 0.004, 0.002, 0.001, 0.02,\n",
    "    0.006, 0.001, 0.002, 0.004, 0.001, 0.002, 0.006, 0.006, 0.004, 0.006, 0.001, 0.002,\n",
    "    0.004, 0.006, 0.006, 0.001, 0.04 , 0.006, 0.002, 0.004, 0.002, 0.002, 0.006, 0.002,\n",
    "    0.002, 0.004, 0.006, 0.006, 0.002, 0.002, 0.008, 0.006, 0.004, 0.002, 0.006, 0.002,\n",
    "    0.004, 0.006, 0.002, 0.004, 0.001, 0.004, 0.002, 0.004, 0.008, 0.006, 0.008, 0.002,\n",
    "    0.004, 0.002, 0.001, 0.004, 0.004, 0.004, 0.006, 0.008, 0.004, 0.001, 0.001, 0.002,\n",
    "    0.006, 0.004, 0.001, 0.002, 0.006, 0.004, 0.006, 0.008, 0.002, 0.002, 0.004, 0.002,\n",
    "    0.04 , 0.002, 0.002, 0.004, 0.002, 0.002, 0.006, 0.02 , 0.004, 0.002, 0.006, 0.02,\n",
    "    0.001, 0.002, 0.006, 0.004, 0.006, 0.004, 0.004, 0.004, 0.004, 0.002, 0.004, 0.04,\n",
    "    0.002, 0.008, 0.002, 0.004, 0.001, 0.004, 0.006, 0.004,\n",
    "]\n",
    "\n",
    "INDEX_WEIGHTS = {int(k):v for k,v in enumerate(INDEX_WEIGHTS)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_return(series: np.ndarray):\n",
    "  return np.log(series).diff()\n",
    "\n",
    "def log_transform(df, cols):\n",
    "  for col in cols:\n",
    "    df['log_{col}'] = df.groupby('stock_id')[col].apply(log_return)\\\n",
    "      .reset_index().sort_values(['level_1', 'stock_id'])[col].values\n",
    "  return df\n",
    "\n",
    "def imbalance_features(df):\n",
    "  prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n",
    "  log_price = [\"log_\"+p for p in prices]\n",
    "  df = log_transform(df, prices)\n",
    "  sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n",
    "\n",
    "  # V1\n",
    "  df[\"volume\"] = df.eval(\"ask_size + bid_size\")\n",
    "  df[\"mid_price\"] = df.eval(\"(ask_price + bid_price) / 2\")\n",
    "  df[\"liquidity_imbalance\"] = df.eval(\"(bid_size-ask_size)/(bid_size+ask_size)\")\n",
    "  df[\"matched_imbalance\"] = df.eval(\"(imbalance_size-matched_size)/(matched_size+imbalance_size)\")\n",
    "  df[\"size_imbalance\"] = df.eval(\"bid_size / ask_size\")\n",
    "  \n",
    "  for c in combinations(prices, 2):\n",
    "    df[f\"{c[0]}_{c[1]}_imb\"] = df.eval(f\"({c[0]} - {c[1]})/({c[0]} + {c[1]})\")\n",
    "\n",
    "  for c in [['ask_price', 'bid_price', 'wap', 'reference_price'], sizes]:\n",
    "    triplet_feature = calculate_triplet_imbalance_numba(c, df)\n",
    "    df[triplet_feature.columns] = triplet_feature.values\n",
    "      \n",
    "  # V2\n",
    "  df[\"index_weights\"] = df[\"stock_id\"].map(INDEX_WEIGHTS)\n",
    "  df[\"weighted_wap\"]  = df[\"stock_weights\"] * df[\"wap\"]\n",
    "  df['wap_momentum']  = df.groupby('stock_id')['weighted_wap'].pct_change(periods=6)\n",
    "  df[\"imbalance_momentum\"] = df.groupby(['stock_id'])['imbalance_size'].diff(periods=1) / df['matched_size']\n",
    "  df[\"price_spread\"]     = df[\"ask_price\"] - df[\"bid_price\"]\n",
    "  df[\"spread_intensity\"] = df.groupby(['stock_id'])['price_spread'].diff()\n",
    "  df['price_pressure'] = df['imbalance_size'] * (df['ask_price'] - df['bid_price'])\n",
    "  df['market_urgency'] = df['price_spread'] * df['liquidity_imbalance']\n",
    "  df['depth_pressure'] = (df['ask_size'] - df['bid_size']) * (df['far_price'] - df['near_price'])\n",
    "  df['spread_depth_ratio'] = (df['ask_price'] - df['bid_price']) / (df['bid_size'] + df['ask_size'])\n",
    "  df['mid_price_movement'] = df['mid_price'].diff(periods=5).apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\n",
    "  df['micro_price']     = ((df['bid_price'] * df['ask_size']) + (df['ask_price'] * df['bid_size'])) / (df['bid_size'] + df['ask_size'])\n",
    "  df['relative_spread'] = (df['ask_price'] - df['bid_price']) / df['wap']\n",
    "  df['volume_imbalance'] = abs((df['ask_size'] - df['bid_size'])) / (df['bid_size'] + df['ask_size'])\n",
    "\n",
    "  df[\"log_mid_price\"] = df.eval(\"(log_ask_price + log_bid_price) / 2\")\n",
    "  df['log_weighted_wap'] = df[\"stock_weights\"] * df[\"log_wap\"]\n",
    "  df['log_mid_price_movement'] = df['log_mid_price'].diff(periods=5).apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\n",
    "  df['log_micro_price'] = ((df['log_bid_price'] * df['ask_size']) + (df['log_ask_price'] * df['bid_size'])) / (df['bid_size'] + df['ask_size'])\n",
    "      \n",
    "  # V3\n",
    "  for col in ['matched_size', 'imbalance_size', 'reference_price']:\n",
    "    for window in [1, 2, 3, 5, 10, 15]:\n",
    "      df[f\"{col}_shift_{window}\"] = df.groupby('stock_id')[col].shift(window)\n",
    "      df[f\"{col}_ret_{window}\"]   = df.groupby('stock_id')[col].pct_change(window)\n",
    "          \n",
    "  for col in prices+log_price+['ask_size', 'bid_size']:\n",
    "    for window in [1, 2, 3, 5, 10, 15]:\n",
    "      df[f\"{col}_diff_{window}\"] = df.groupby(\"stock_id\")[col].diff(window)\n",
    "  return df.replace([np.inf, -np.inf], 0)\n",
    "\n",
    "# generate time features\n",
    "def other_features(df):\n",
    "  df[\"dow\"] = df[\"date_id\"] % 5\n",
    "  df[\"dom\"] = df[\"date_id\"] % 20\n",
    "  df[\"seconds\"] = df[\"seconds_in_bucket\"] % 60\n",
    "  df[\"minute\"]  = df[\"seconds_in_bucket\"] // 60\n",
    "  df['last_imbalance_change'] = calculate_last_imbalance_change((df['sign_imbalance_size']>=0).values)\n",
    "  return df\n",
    "\n",
    "# generate all features\n",
    "def generate_all_features(df):\n",
    "  cols = [c for c in df.columns if c not in [\"row_id\", \"time_id\", \"target\"]]\n",
    "  df = df[cols]\n",
    "  df = imbalance_features(df)\n",
    "  df = other_features(df)\n",
    "  gc.collect()\n",
    "  feature_name = [i for i in df.columns if i not in [\"row_id\", \"target\", \"time_id\", \"date_id\"]]\n",
    "  return df[feature_name]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROLLING IDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "class CustomAvg:\n",
    "  def __init__(self, capacity, alpha=0.75):\n",
    "    self.weights = alpha * (1-alpha)**np.arange(0, capacity)\n",
    "    self.weights /= self.weights.sum()\n",
    "    self.__name__ = \"customAvg\"\n",
    "  def __call__(self, x): # x must be of length == capacity\n",
    "    return np.average(x, weights=self.weights)\n",
    "  \n",
    "def realized_volatility(series):\n",
    "  return np.sqrt(np.sum(series**2))\n",
    "   \n",
    "def generate_rolling_features(df):\n",
    "  prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n",
    "  log_prices = ['log_'+i for i in prices]\n",
    "  sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\", \"sign_imbalance_size\"]\n",
    "  extra = ['wap_momentum','imbalance_momentum','price_spread','spread_intensity',\n",
    "           'price_pressure','market_urgency','depth_pressure','spread_depth_ratio',\n",
    "           'mid_price_movement','relative_spread', 'volume_imbalance', 'log_mid_price',\n",
    "           'log_weighted_wap','log_mid_price_movement','log_micro_price',\n",
    "           ]\n",
    "  for window in [3, 5, 7, 10, 15, 20]:\n",
    "    for func in [np.sum, np.mean, np.std, skew, kurtosis, realized_volatility, CustomAvg(window)]:\n",
    "      for col in log_prices+sizes+extra:\n",
    "        df[f\"rolling_{col}_{func.__name__}_{window}\"] = df.groupby('stock_id')[col].rolling(window).agg(func).\\\n",
    "          reset_index().sort_values(['level_1', 'stock_id'])[col].values\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RANK IDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "def rank_features(df, cols):\n",
    "  for col in cols:\n",
    "    df[f'{col}_rank'] = df.groupby('time_id')[col].rank(pct=True)\n",
    "  return df\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGBT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "def rmspe(y_true, y_pred):\n",
    "  return  (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))\n",
    "\n",
    "def feval_RMSPE(preds, train_data):\n",
    "  labels = train_data.get_label()\n",
    "  return 'RMSPE', round(rmspe(y_true = labels, y_pred = preds),5), False\n",
    "\n",
    "def plot_importance(cvbooster, figsize=(10, 10)):\n",
    "  raw_importances = cvbooster.feature_importance(importance_type='gain')\n",
    "  feature_name = cvbooster.boosters[0].feature_name()\n",
    "  importance_df = pd.DataFrame(data=raw_importances,\n",
    "                                columns=feature_name)\n",
    "  # 平均値でソートする\n",
    "  sorted_indices = importance_df.mean(axis=0).sort_values(ascending=False).index\n",
    "  sorted_importance_df = importance_df.loc[:, sorted_indices]\n",
    "  # 上位をプロットする\n",
    "  PLOT_TOP_N = 80\n",
    "  plot_cols = sorted_importance_df.columns[:PLOT_TOP_N]\n",
    "  _, ax = plt.subplots(figsize=figsize)\n",
    "  ax.grid()\n",
    "  ax.set_xscale('log')\n",
    "  ax.set_ylabel('Feature')\n",
    "  ax.set_xlabel('Importance')\n",
    "  sns.boxplot(data=sorted_importance_df[plot_cols],\n",
    "              orient='h',\n",
    "              ax=ax)\n",
    "  plt.show()\n",
    "\n",
    "lr = GBDT_LR\n",
    "params = {\n",
    "  'objective': 'regression',\n",
    "  'verbose': 0,\n",
    "  'metric': '',\n",
    "  'reg_alpha': 5,\n",
    "  'reg_lambda': 5,\n",
    "  'min_data_in_leaf': 1000,\n",
    "  'max_depth': -1,\n",
    "  'num_leaves': 128,\n",
    "  'colsample_bytree': 0.3,\n",
    "  'learning_rate': lr\n",
    "}\n",
    "\n",
    "class EnsembleModel:\n",
    "  def __init__(self, models: List[lgb.Booster], weights: Optional[List[float]] = None):\n",
    "    self.models = models\n",
    "    self.weights = weights\n",
    "\n",
    "    features = list(self.models[0].feature_name())\n",
    "\n",
    "    for m in self.models[1:]:\n",
    "      assert features == list(m.feature_name())\n",
    "\n",
    "  def predict(self, x):\n",
    "    predicted = np.zeros((len(x), len(self.models)))\n",
    "\n",
    "    for i, m in enumerate(self.models):\n",
    "      w = self.weights[i] if self.weights is not None else 1\n",
    "      predicted[:, i] = w * m.predict(x)\n",
    "\n",
    "    ttl = np.sum(self.weights) if self.weights is not None else len(self.models)\n",
    "    return np.sum(predicted, axis=1) / ttl\n",
    "\n",
    "  def feature_name(self) -> List[str]:\n",
    "    return self.models[0].feature_name()\n",
    "\n",
    "def get_X(df_src):\n",
    "  cols = [c for c in df_src.columns if c not in ['time_id', 'target']]\n",
    "  return df_src[cols]\n",
    "\n",
    "X = get_X(df_train)\n",
    "y = df_train['target']\n",
    "X.to_feather('X.f')\n",
    "df_train[['target']].to_feather('y.f')\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "if PREDICT_GBDT:\n",
    "  ds = lgb.Dataset(X, y, weight=1/np.power(y, 2))\n",
    "\n",
    "  with timer('lgb.cv'):\n",
    "    ret = lgb.cv(params, ds, num_boost_round=8000, folds=folds, #cv,\n",
    "                  feval=feval_RMSPE, stratified=False, \n",
    "                  return_cvbooster=True, verbose_eval=20,\n",
    "                  early_stopping_rounds=int(40*0.1/lr))\n",
    "\n",
    "    # current best: 0.204666 (cluster groupkfold)\n",
    "    # current best: 0.20629 (time-series, 4folds, 10% valid)\n",
    "    print(f\"# overall RMSPE: {ret['RMSPE-mean'][-1]}\")\n",
    "\n",
    "  best_iteration = len(ret['RMSPE-mean'])\n",
    "  for i in range(len(folds)):\n",
    "    y_pred = ret['cvbooster'].boosters[i].predict(X.iloc[folds[i][1]], num_iteration=best_iteration)\n",
    "    y_true = y.iloc[folds[i][1]]\n",
    "    print(f\"# fold{i} RMSPE: {rmspe(y_true, y_pred)}\")\n",
    "    \n",
    "    if i == len(folds) - 1:\n",
    "        np.save('fold3_pred_gbdt.npy', y_pred)\n",
    "\n",
    "  plot_importance(ret['cvbooster'], figsize=(10, 20))\n",
    "\n",
    "\n",
    "  boosters = []\n",
    "  with timer('retraining'):\n",
    "    for _ in range(GBDT_NUM_MODELS):\n",
    "      boosters.append(lgb.train(params, ds, num_boost_round=int(1.1*best_iteration)))\n",
    "\n",
    "  booster = EnsembleModel(boosters)\n",
    "  del ret\n",
    "\n",
    "  #del X, y, ds\n",
    "  del ds\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN cringe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "from typing import List, Tuple, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.special import erf, erfinv\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils.validation import FLOAT_DTYPES, check_array, check_is_fitted\n",
    "from sklearn.decomposition import PCA\n",
    "from pytorch_tabnet.metrics import Metric\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts\n",
    "\n",
    "\n",
    "null_check_cols = [\n",
    "    'book.log_return1.realized_volatility',\n",
    "    'book_150.log_return1.realized_volatility',\n",
    "    'book_300.log_return1.realized_volatility',\n",
    "    'book_450.log_return1.realized_volatility',\n",
    "    'trade.log_return.realized_volatility',\n",
    "    'trade_150.log_return.realized_volatility',\n",
    "    'trade_300.log_return.realized_volatility',\n",
    "    'trade_450.log_return.realized_volatility'\n",
    "]\n",
    "\n",
    "class GaussRankScaler(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Transform features by scaling each feature to a normal distribution.\n",
    "    Parameters\n",
    "        ----------\n",
    "        epsilon : float, optional, default 1e-4\n",
    "            A small amount added to the lower bound or subtracted\n",
    "            from the upper bound. This value prevents infinite number\n",
    "            from occurring when applying the inverse error function.\n",
    "        copy : boolean, optional, default True\n",
    "            If False, try to avoid a copy and do inplace scaling instead.\n",
    "            This is not guaranteed to always work inplace; e.g. if the data is\n",
    "            not a NumPy array, a copy may still be returned.\n",
    "        n_jobs : int or None, optional, default None\n",
    "            Number of jobs to run in parallel.\n",
    "            ``None`` means 1 and ``-1`` means using all processors.\n",
    "        interp_kind : str or int, optional, default 'linear'\n",
    "           Specifies the kind of interpolation as a string\n",
    "            ('linear', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic',\n",
    "            'previous', 'next', where 'zero', 'slinear', 'quadratic' and 'cubic'\n",
    "            refer to a spline interpolation of zeroth, first, second or third\n",
    "            order; 'previous' and 'next' simply return the previous or next value\n",
    "            of the point) or as an integer specifying the order of the spline\n",
    "            interpolator to use.\n",
    "        interp_copy : bool, optional, default False\n",
    "            If True, the interpolation function makes internal copies of x and y.\n",
    "            If False, references to `x` and `y` are used.\n",
    "        Attributes\n",
    "        ----------\n",
    "        interp_func_ : list\n",
    "            The interpolation function for each feature in the training set.\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self, epsilon=1e-4, copy=True, n_jobs=None, interp_kind='linear', interp_copy=False):\n",
    "        self.epsilon = epsilon\n",
    "        self.copy = copy\n",
    "        self.interp_kind = interp_kind\n",
    "        self.interp_copy = interp_copy\n",
    "        self.fill_value = 'extrapolate'\n",
    "        self.n_jobs = n_jobs\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit interpolation function to link rank with original data for future scaling\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            The data used to fit interpolation function for later scaling along the features axis.\n",
    "        y\n",
    "            Ignored\n",
    "        \"\"\"\n",
    "        X = check_array(X, copy=self.copy, estimator=self, dtype=FLOAT_DTYPES, force_all_finite=True)\n",
    "\n",
    "        self.interp_func_ = Parallel(n_jobs=self.n_jobs)(delayed(self._fit)(x) for x in X.T)\n",
    "        return self\n",
    "\n",
    "    def _fit(self, x):\n",
    "        x = self.drop_duplicates(x)\n",
    "        rank = np.argsort(np.argsort(x))\n",
    "        bound = 1.0 - self.epsilon\n",
    "        factor = np.max(rank) / 2.0 * bound\n",
    "        scaled_rank = np.clip(rank / factor - bound, -bound, bound)\n",
    "        return interp1d(\n",
    "            x, scaled_rank, kind=self.interp_kind, copy=self.interp_copy, fill_value=self.fill_value)\n",
    "\n",
    "    def transform(self, X, copy=None):\n",
    "        \"\"\"Scale the data with the Gauss Rank algorithm\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            The data used to scale along the features axis.\n",
    "        copy : bool, optional (default: None)\n",
    "            Copy the input X or not.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, 'interp_func_')\n",
    "\n",
    "        copy = copy if copy is not None else self.copy\n",
    "        X = check_array(X, copy=copy, estimator=self, dtype=FLOAT_DTYPES, force_all_finite=True)\n",
    "\n",
    "        X = np.array(Parallel(n_jobs=self.n_jobs)(delayed(self._transform)(i, x) for i, x in enumerate(X.T))).T\n",
    "        return X\n",
    "\n",
    "    def _transform(self, i, x):\n",
    "        return erfinv(self.interp_func_[i](x))\n",
    "\n",
    "    def inverse_transform(self, X, copy=None):\n",
    "        \"\"\"Scale back the data to the original representation\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape [n_samples, n_features]\n",
    "            The data used to scale along the features axis.\n",
    "        copy : bool, optional (default: None)\n",
    "            Copy the input X or not.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, 'interp_func_')\n",
    "\n",
    "        copy = copy if copy is not None else self.copy\n",
    "        X = check_array(X, copy=copy, estimator=self, dtype=FLOAT_DTYPES, force_all_finite=True)\n",
    "\n",
    "        X = np.array(Parallel(n_jobs=self.n_jobs)(delayed(self._inverse_transform)(i, x) for i, x in enumerate(X.T))).T\n",
    "        return X\n",
    "\n",
    "    def _inverse_transform(self, i, x):\n",
    "        inv_interp_func = interp1d(self.interp_func_[i].y, self.interp_func_[i].x, kind=self.interp_kind,\n",
    "                                   copy=self.interp_copy, fill_value=self.fill_value)\n",
    "        return inv_interp_func(erf(x))\n",
    "\n",
    "    @staticmethod\n",
    "    def drop_duplicates(x):\n",
    "        is_unique = np.zeros_like(x, dtype=bool)\n",
    "        is_unique[np.unique(x, return_index=True)[1]] = True\n",
    "        return x[is_unique]\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def rmspe_metric(y_true, y_pred):\n",
    "    rmspe = np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "    return rmspe\n",
    "\n",
    "\n",
    "def rmspe_loss(y_true, y_pred):\n",
    "    rmspe = torch.sqrt(torch.mean(torch.square((y_true - y_pred) / y_true)))\n",
    "    return rmspe\n",
    "\n",
    "\n",
    "class RMSPE(Metric):\n",
    "    def __init__(self):\n",
    "        self._name = \"rmspe\"\n",
    "        self._maximize = False\n",
    "\n",
    "    def __call__(self, y_true, y_score):\n",
    "        return np.sqrt(np.mean(np.square((y_true - y_score) / y_true)))\n",
    "\n",
    "def RMSPELoss_Tabnet(y_pred, y_true):\n",
    "    return torch.sqrt(torch.mean( ((y_true - y_pred) / y_true) ** 2 )).clone()\n",
    "\n",
    "\n",
    "class AverageMeter:\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, x_num: np.ndarray, x_cat: np.ndarray, y: Optional[np.ndarray]):\n",
    "        super().__init__()\n",
    "        self.x_num = x_num\n",
    "        self.x_cat = x_cat\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_num)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is None:\n",
    "            return self.x_num[idx], torch.LongTensor(self.x_cat[idx])\n",
    "        else:\n",
    "            return self.x_num[idx], torch.LongTensor(self.x_cat[idx]), self.y[idx]\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 src_num_dim: int,\n",
    "                 n_categories: List[int],\n",
    "                 dropout: float = 0.0,\n",
    "                 hidden: int = 50,\n",
    "                 emb_dim: int = 10,\n",
    "                 dropout_cat: float = 0.2,\n",
    "                 bn: bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embs = nn.ModuleList([\n",
    "            nn.Embedding(x, emb_dim) for x in n_categories])\n",
    "        self.cat_dim = emb_dim * len(n_categories)\n",
    "        self.dropout_cat = nn.Dropout(dropout_cat)\n",
    "\n",
    "        if bn:\n",
    "            self.sequence = nn.Sequential(\n",
    "                nn.Linear(src_num_dim + self.cat_dim, hidden),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.BatchNorm1d(hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden, hidden),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.BatchNorm1d(hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden, 1)\n",
    "            )\n",
    "        else:\n",
    "            self.sequence = nn.Sequential(\n",
    "                nn.Linear(src_num_dim + self.cat_dim, hidden),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden, hidden),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden, 1)\n",
    "            )\n",
    "\n",
    "    def forward(self, x_num, x_cat):\n",
    "        embs = [embedding(x_cat[:, i]) for i, embedding in enumerate(self.embs)]\n",
    "        x_cat_emb = self.dropout_cat(torch.cat(embs, 1))\n",
    "        x_all = torch.cat([x_num, x_cat_emb], 1)\n",
    "        x = self.sequence(x_all)\n",
    "        return torch.squeeze(x)\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_features: int,\n",
    "                 hidden_size: int,\n",
    "                 n_categories: List[int],\n",
    "                 emb_dim: int = 10,\n",
    "                 dropout_cat: float = 0.2,\n",
    "                 channel_1: int = 256,\n",
    "                 channel_2: int = 512,\n",
    "                 channel_3: int = 512,\n",
    "                 dropout_top: float = 0.1,\n",
    "                 dropout_mid: float = 0.3,\n",
    "                 dropout_bottom: float = 0.2,\n",
    "                 weight_norm: bool = True,\n",
    "                 two_stage: bool = True,\n",
    "                 celu: bool = True,\n",
    "                 kernel1: int = 5,\n",
    "                 leaky_relu: bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "        num_targets = 1\n",
    "\n",
    "        cha_1_reshape = int(hidden_size / channel_1)\n",
    "        cha_po_1 = int(hidden_size / channel_1 / 2)\n",
    "        cha_po_2 = int(hidden_size / channel_1 / 2 / 2) * channel_3\n",
    "\n",
    "        self.cat_dim = emb_dim * len(n_categories)\n",
    "        self.cha_1 = channel_1\n",
    "        self.cha_2 = channel_2\n",
    "        self.cha_3 = channel_3\n",
    "        self.cha_1_reshape = cha_1_reshape\n",
    "        self.cha_po_1 = cha_po_1\n",
    "        self.cha_po_2 = cha_po_2\n",
    "        self.two_stage = two_stage\n",
    "\n",
    "        self.expand = nn.Sequential(\n",
    "            nn.BatchNorm1d(num_features + self.cat_dim),\n",
    "            nn.Dropout(dropout_top),\n",
    "            nn.utils.weight_norm(nn.Linear(num_features + self.cat_dim, hidden_size), dim=None),\n",
    "            nn.CELU(0.06) if celu else nn.ReLU()\n",
    "        )\n",
    "\n",
    "        def _norm(layer, dim=None):\n",
    "            return nn.utils.weight_norm(layer, dim=dim) if weight_norm else layer\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.BatchNorm1d(channel_1),\n",
    "            nn.Dropout(dropout_top),\n",
    "            _norm(nn.Conv1d(channel_1, channel_2, kernel_size=kernel1, stride=1, padding=kernel1 // 2, bias=False)),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(output_size=cha_po_1),\n",
    "            nn.BatchNorm1d(channel_2),\n",
    "            nn.Dropout(dropout_top),\n",
    "            _norm(nn.Conv1d(channel_2, channel_2, kernel_size=3, stride=1, padding=1, bias=True)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        if self.two_stage:\n",
    "            self.conv2 = nn.Sequential(\n",
    "                nn.BatchNorm1d(channel_2),\n",
    "                nn.Dropout(dropout_mid),\n",
    "                _norm(nn.Conv1d(channel_2, channel_2, kernel_size=3, stride=1, padding=1, bias=True)),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(channel_2),\n",
    "                nn.Dropout(dropout_bottom),\n",
    "                _norm(nn.Conv1d(channel_2, channel_3, kernel_size=5, stride=1, padding=2, bias=True)),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "\n",
    "        self.max_po_c2 = nn.MaxPool1d(kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "        self.flt = nn.Flatten()\n",
    "\n",
    "        if leaky_relu:\n",
    "            self.dense = nn.Sequential(\n",
    "                nn.BatchNorm1d(cha_po_2),\n",
    "                nn.Dropout(dropout_bottom),\n",
    "                _norm(nn.Linear(cha_po_2, num_targets), dim=0),\n",
    "                nn.LeakyReLU()\n",
    "            )\n",
    "        else:\n",
    "            self.dense = nn.Sequential(\n",
    "                nn.BatchNorm1d(cha_po_2),\n",
    "                nn.Dropout(dropout_bottom),\n",
    "                _norm(nn.Linear(cha_po_2, num_targets), dim=0)\n",
    "            )\n",
    "\n",
    "        self.embs = nn.ModuleList([nn.Embedding(x, emb_dim) for x in n_categories])\n",
    "        self.cat_dim = emb_dim * len(n_categories)\n",
    "        self.dropout_cat = nn.Dropout(dropout_cat)\n",
    "\n",
    "    def forward(self, x_num, x_cat):\n",
    "        embs = [embedding(x_cat[:, i]) for i, embedding in enumerate(self.embs)]\n",
    "        x_cat_emb = self.dropout_cat(torch.cat(embs, 1))\n",
    "        x = torch.cat([x_num, x_cat_emb], 1)\n",
    "\n",
    "        x = self.expand(x)\n",
    "\n",
    "        x = x.reshape(x.shape[0], self.cha_1, self.cha_1_reshape)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        if self.two_stage:\n",
    "            x = self.conv2(x) * x\n",
    "\n",
    "        x = self.max_po_c2(x)\n",
    "        x = self.flt(x)\n",
    "        x = self.dense(x)\n",
    "\n",
    "        return torch.squeeze(x)\n",
    "\n",
    "\n",
    "def preprocess_nn(\n",
    "        X: pd.DataFrame,\n",
    "        scaler: Optional[StandardScaler] = None,\n",
    "        scaler_type: str = 'standard',\n",
    "        n_pca: int = -1,\n",
    "        na_cols: bool = True):\n",
    "    if na_cols:\n",
    "        #for c in X.columns:\n",
    "        for c in null_check_cols:\n",
    "            if c in X.columns:\n",
    "                X[f\"{c}_isnull\"] = X[c].isnull().astype(int)\n",
    "\n",
    "    cat_cols = [c for c in X.columns if c in ['time_id', 'stock_id']]\n",
    "    num_cols = [c for c in X.columns if c not in cat_cols]\n",
    "\n",
    "    X_num = X[num_cols].values.astype(np.float32)\n",
    "    X_cat = np.nan_to_num(X[cat_cols].values.astype(np.int32))\n",
    "\n",
    "    def _pca(X_num_):\n",
    "        if n_pca > 0:\n",
    "            pca = PCA(n_components=n_pca, random_state=0)\n",
    "            return pca.fit_transform(X_num)\n",
    "        return X_num\n",
    "\n",
    "    if scaler is None:\n",
    "        if scaler_type == 'standard':\n",
    "            scaler = StandardScaler()\n",
    "        elif scaler_type == 'gauss':\n",
    "            scaler = GaussRankScaler()\n",
    "            X_num = np.nan_to_num(X_num, posinf=0, neginf=0)\n",
    "        X_num = scaler.fit_transform(X_num)\n",
    "        X_num = np.nan_to_num(X_num, posinf=0, neginf=0)\n",
    "        return _pca(X_num), X_cat, cat_cols, scaler\n",
    "    else:\n",
    "        X_num = scaler.transform(X_num) #TODO: infでも大丈夫？\n",
    "        X_num = np.nan_to_num(X_num, posinf=0, neginf=0)\n",
    "        return _pca(X_num), X_cat, cat_cols\n",
    "\n",
    "\n",
    "def train_epoch(data_loader: DataLoader,\n",
    "                model: nn.Module,\n",
    "                optimizer,\n",
    "                scheduler,\n",
    "                device,\n",
    "                clip_grad: float = 1.5):\n",
    "    model.train()\n",
    "    losses = AverageMeter()\n",
    "    step = 0\n",
    "\n",
    "    for x_num, x_cat, y in tqdm(data_loader, position=0, leave=True, desc='Training'):\n",
    "        batch_size = x_num.size(0)\n",
    "        x_num = x_num.to(device, dtype=torch.float)\n",
    "        x_cat = x_cat.to(device)\n",
    "        y = y.to(device, dtype=torch.float)\n",
    "\n",
    "        loss = rmspe_loss(y, model(x_num, x_cat))\n",
    "        losses.update(loss.detach().cpu().numpy(), batch_size)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        step += 1\n",
    "\n",
    "    return losses.avg\n",
    "\n",
    "\n",
    "def evaluate(data_loader: DataLoader, model, device):\n",
    "    model.eval()\n",
    "\n",
    "    losses = AverageMeter()\n",
    "\n",
    "    final_targets = []\n",
    "    final_outputs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_num, x_cat, y in tqdm(data_loader, position=0, leave=True, desc='Evaluating'):\n",
    "            batch_size = x_num.size(0)\n",
    "            x_num = x_num.to(device, dtype=torch.float)\n",
    "            x_cat = x_cat.to(device)\n",
    "            y = y.to(device, dtype=torch.float)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model(x_num, x_cat)\n",
    "\n",
    "            loss = rmspe_loss(y, output)\n",
    "            # record loss\n",
    "            losses.update(loss.detach().cpu().numpy(), batch_size)\n",
    "\n",
    "            targets = y.detach().cpu().numpy()\n",
    "            output = output.detach().cpu().numpy()\n",
    "\n",
    "            final_targets.append(targets)\n",
    "            final_outputs.append(output)\n",
    "\n",
    "    final_targets = np.concatenate(final_targets)\n",
    "    final_outputs = np.concatenate(final_outputs)\n",
    "\n",
    "    try:\n",
    "        metric = rmspe_metric(final_targets, final_outputs)\n",
    "    except:\n",
    "        metric = None\n",
    "\n",
    "    return final_outputs, final_targets, losses.avg, metric\n",
    "\n",
    "\n",
    "def predict_nn(X: pd.DataFrame,\n",
    "               model: Union[List[MLP], MLP],\n",
    "               scaler: StandardScaler,\n",
    "               device,\n",
    "               ensemble_method='mean'):\n",
    "    if not isinstance(model, list):\n",
    "        model = [model]\n",
    "\n",
    "    for m in model:\n",
    "        m.eval()\n",
    "    X_num, X_cat, cat_cols = preprocess_nn(X.copy(), scaler=scaler)\n",
    "    valid_dataset = TabularDataset(X_num, X_cat, None)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_dataset,\n",
    "                                               batch_size=512,\n",
    "                                               shuffle=False,\n",
    "                                               num_workers=4)\n",
    "\n",
    "    final_outputs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_num, x_cat in tqdm(valid_loader, position=0, leave=True, desc='Evaluating'):\n",
    "            x_num = x_num.to(device, dtype=torch.float)\n",
    "            x_cat = x_cat.to(device)\n",
    "\n",
    "            outputs = []\n",
    "            with torch.no_grad():\n",
    "                for m in model:\n",
    "                    output = m(x_num, x_cat)\n",
    "                    outputs.append(output.detach().cpu().numpy())\n",
    "\n",
    "            if ensemble_method == 'median':\n",
    "                pred = np.nanmedian(np.array(outputs), axis=0)\n",
    "            else:\n",
    "                pred = np.array(outputs).mean(axis=0)\n",
    "            final_outputs.append(pred)\n",
    "\n",
    "    final_outputs = np.concatenate(final_outputs)\n",
    "    return final_outputs\n",
    "\n",
    "\n",
    "def predict_tabnet(X: pd.DataFrame,\n",
    "                   model: Union[List[TabNetRegressor], TabNetRegressor],\n",
    "                   scaler: StandardScaler,\n",
    "                   ensemble_method='mean'):\n",
    "    if not isinstance(model, list):\n",
    "        model = [model]\n",
    "\n",
    "    X_num, X_cat, cat_cols = preprocess_nn(X.copy(), scaler=scaler)\n",
    "    X_processed = np.concatenate([X_num, X_cat], axis=1)\n",
    "\n",
    "    predicted = []\n",
    "    for m in model:\n",
    "        predicted.append(m.predict(X_processed))\n",
    "\n",
    "    if ensemble_method == 'median':\n",
    "        pred = np.nanmedian(np.array(predicted), axis=0)\n",
    "    else:\n",
    "        pred = np.array(predicted).mean(axis=0)\n",
    "\n",
    "    return pred\n",
    "\n",
    "\n",
    "def train_tabnet(X: pd.DataFrame,\n",
    "                 y: pd.DataFrame,\n",
    "                 folds: List[Tuple],\n",
    "                 batch_size: int = 1024,\n",
    "                 lr: float = 1e-3,\n",
    "                 model_path: str = 'fold_{}.pth',\n",
    "                 scaler_type: str = 'standard',\n",
    "                 output_dir: str = 'artifacts',\n",
    "                 epochs: int = 250,\n",
    "                 seed: int = 42,\n",
    "                 n_pca: int = -1,\n",
    "                 na_cols: bool = True,\n",
    "                 patience: int = 10,\n",
    "                 factor: float = 0.5,\n",
    "                 gamma: float = 2.0,\n",
    "                 lambda_sparse: float = 8.0,\n",
    "                 n_steps: int = 2,\n",
    "                 scheduler_type: str = 'cosine'):\n",
    "    seed_everything(seed)\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    y = y.values.astype(np.float32)\n",
    "    X_num, X_cat, cat_cols, scaler = preprocess_nn(X.copy(), scaler_type=scaler_type, n_pca=n_pca, na_cols=na_cols)\n",
    "\n",
    "    best_losses = []\n",
    "    best_predictions = []\n",
    "\n",
    "    for cv_idx, (train_idx, valid_idx) in enumerate(folds):\n",
    "        X_tr, X_va = X_num[train_idx], X_num[valid_idx]\n",
    "        X_tr_cat, X_va_cat = X_cat[train_idx], X_cat[valid_idx]\n",
    "        y_tr, y_va = y[train_idx], y[valid_idx]\n",
    "        y_tr = y_tr.reshape(-1,1)\n",
    "        y_va = y_va.reshape(-1,1)\n",
    "        X_tr = np.concatenate([X_tr_cat, X_tr], axis=1)\n",
    "        X_va = np.concatenate([X_va_cat, X_va], axis=1)\n",
    "\n",
    "        cat_idxs = [0]\n",
    "        cat_dims = [128]\n",
    "\n",
    "        if scheduler_type == 'cosine':\n",
    "            scheduler_params = dict(T_0=200, T_mult=1, eta_min=1e-4, last_epoch=-1, verbose=False)\n",
    "            scheduler_fn = CosineAnnealingWarmRestarts\n",
    "        else:\n",
    "            scheduler_params = {'mode': 'min', 'min_lr': 1e-7, 'patience': patience, 'factor': factor, 'verbose': True}\n",
    "            scheduler_fn = torch.optim.lr_scheduler.ReduceLROnPlateau\n",
    "\n",
    "        model = TabNetRegressor(\n",
    "            cat_idxs=cat_idxs,\n",
    "            cat_dims=cat_dims,\n",
    "            cat_emb_dim=1,\n",
    "            n_d=16,\n",
    "            n_a=16,\n",
    "            n_steps=n_steps,\n",
    "            gamma=gamma,\n",
    "            n_independent=2,\n",
    "            n_shared=2,\n",
    "            lambda_sparse=lambda_sparse,\n",
    "            optimizer_fn=torch.optim.Adam,\n",
    "            optimizer_params={'lr': lr},\n",
    "            mask_type=\"entmax\",\n",
    "            scheduler_fn=scheduler_fn,\n",
    "            scheduler_params=scheduler_params,\n",
    "            seed=seed,\n",
    "            verbose=10\n",
    "            #device_name=device,\n",
    "            #clip_value=1.5\n",
    "        )\n",
    "\n",
    "        model.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], max_epochs=epochs, patience=50, batch_size=1024*20,\n",
    "                  virtual_batch_size=batch_size, num_workers=4, drop_last=False, eval_metric=[RMSPE], loss_fn=RMSPELoss_Tabnet)\n",
    "\n",
    "        path = os.path.join(output_dir, model_path.format(cv_idx))\n",
    "        model.save_model(path)\n",
    "\n",
    "        predicted = model.predict(X_va)\n",
    "\n",
    "        rmspe = rmspe_metric(y_va, predicted)\n",
    "        best_losses.append(rmspe)\n",
    "        best_predictions.append(predicted)\n",
    "\n",
    "    return best_losses, best_predictions, scaler, model\n",
    "\n",
    "\n",
    "def train_nn(X: pd.DataFrame,\n",
    "             y: pd.DataFrame,\n",
    "             folds: List[Tuple],\n",
    "             device,\n",
    "             emb_dim: int = 25,\n",
    "             batch_size: int = 1024,\n",
    "             model_type: str = 'mlp',\n",
    "             mlp_dropout: float = 0.0,\n",
    "             mlp_hidden: int = 64,\n",
    "             mlp_bn: bool = False,\n",
    "             cnn_hidden: int = 64,\n",
    "             cnn_channel1: int = 32,\n",
    "             cnn_channel2: int = 32,\n",
    "             cnn_channel3: int = 32,\n",
    "             cnn_kernel1: int = 5,\n",
    "             cnn_celu: bool = False,\n",
    "             cnn_weight_norm: bool = False,\n",
    "             dropout_emb: bool = 0.0,\n",
    "             lr: float = 1e-3,\n",
    "             weight_decay: float = 0.0,\n",
    "             model_path: str = 'fold_{}.pth',\n",
    "             scaler_type: str = 'standard',\n",
    "             output_dir: str = 'artifacts',\n",
    "             scheduler_type: str = 'onecycle',\n",
    "             optimizer_type: str = 'adam',\n",
    "             max_lr: float = 0.01,\n",
    "             epochs: int = 30,\n",
    "             seed: int = 42,\n",
    "             n_pca: int = -1,\n",
    "             batch_double_freq: int = 50,\n",
    "             cnn_dropout: float = 0.1,\n",
    "             na_cols: bool = True,\n",
    "             cnn_leaky_relu: bool = False,\n",
    "             patience: int = 8,\n",
    "             factor: float = 0.5):\n",
    "    seed_everything(seed)\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    y = y.values.astype(np.float32)\n",
    "    X_num, X_cat, cat_cols, scaler = preprocess_nn(X.copy(), scaler_type=scaler_type, n_pca=n_pca, na_cols=na_cols)\n",
    "\n",
    "    best_losses = []\n",
    "    best_predictions = []\n",
    "\n",
    "    for cv_idx, (train_idx, valid_idx) in enumerate(folds):\n",
    "        X_tr, X_va = X_num[train_idx], X_num[valid_idx]\n",
    "        X_tr_cat, X_va_cat = X_cat[train_idx], X_cat[valid_idx]\n",
    "        y_tr, y_va = y[train_idx], y[valid_idx]\n",
    "\n",
    "        cur_batch = batch_size\n",
    "        best_loss = 1e10\n",
    "        best_prediction = None\n",
    "\n",
    "        print(f\"fold {cv_idx} train: {X_tr.shape}, valid: {X_va.shape}\")\n",
    "\n",
    "        train_dataset = TabularDataset(X_tr, X_tr_cat, y_tr)\n",
    "        valid_dataset = TabularDataset(X_va, X_va_cat, y_va)\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=cur_batch, shuffle=True,\n",
    "                                                   num_workers=4)\n",
    "        valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=cur_batch, shuffle=False,\n",
    "                                                   num_workers=4)\n",
    "\n",
    "        if model_type == 'mlp':\n",
    "            model = MLP(X_tr.shape[1],\n",
    "                        n_categories=[128],\n",
    "                        dropout=mlp_dropout, hidden=mlp_hidden, emb_dim=emb_dim,\n",
    "                        dropout_cat=dropout_emb, bn=mlp_bn)\n",
    "        elif model_type == 'cnn':\n",
    "            model = CNN(X_tr.shape[1],\n",
    "                        hidden_size=cnn_hidden,\n",
    "                        n_categories=[128],\n",
    "                        emb_dim=emb_dim,\n",
    "                        dropout_cat=dropout_emb,\n",
    "                        channel_1=cnn_channel1,\n",
    "                        channel_2=cnn_channel2,\n",
    "                        channel_3=cnn_channel3,\n",
    "                        two_stage=False,\n",
    "                        kernel1=cnn_kernel1,\n",
    "                        celu=cnn_celu,\n",
    "                        dropout_top=cnn_dropout,\n",
    "                        dropout_mid=cnn_dropout,\n",
    "                        dropout_bottom=cnn_dropout,\n",
    "                        weight_norm=cnn_weight_norm,\n",
    "                        leaky_relu=cnn_leaky_relu)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "        model = model.to(device)\n",
    "\n",
    "        if optimizer_type == 'adamw':\n",
    "            opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        elif optimizer_type == 'adam':\n",
    "            opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        scheduler = epoch_scheduler = None\n",
    "        if scheduler_type == 'onecycle':\n",
    "            scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer=opt, pct_start=0.1, div_factor=1e3,\n",
    "                                                            max_lr=max_lr, epochs=epochs,\n",
    "                                                            steps_per_epoch=len(train_loader))\n",
    "        elif scheduler_type == 'reduce':\n",
    "            epoch_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=opt,\n",
    "                                                                         mode='min',\n",
    "                                                                         min_lr=1e-7,\n",
    "                                                                         patience=patience,\n",
    "                                                                         verbose=True,\n",
    "                                                                         factor=factor)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            if epoch > 0 and epoch % batch_double_freq == 0:\n",
    "                cur_batch = cur_batch * 2\n",
    "                print(f'batch: {cur_batch}')\n",
    "                train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                                           batch_size=cur_batch,\n",
    "                                                           shuffle=True,\n",
    "                                                           num_workers=4)\n",
    "            train_loss = train_epoch(train_loader, model, opt, scheduler, device)\n",
    "            predictions, valid_targets, valid_loss, rmspe = evaluate(valid_loader, model, device=device)\n",
    "            print(f\"epoch {epoch}, train loss: {train_loss:.3f}, valid rmspe: {rmspe:.3f}\")\n",
    "\n",
    "            if epoch_scheduler is not None:\n",
    "                epoch_scheduler.step(rmspe)\n",
    "\n",
    "            if rmspe < best_loss:\n",
    "                print(f'new best:{rmspe}')\n",
    "                best_loss = rmspe\n",
    "                best_prediction = predictions\n",
    "                torch.save(model, os.path.join(output_dir, model_path.format(cv_idx)))\n",
    "\n",
    "        best_predictions.append(best_prediction)\n",
    "        best_losses.append(best_loss)\n",
    "        del model, train_dataset, valid_dataset, train_loader, valid_loader, X_tr, X_va, X_tr_cat, X_va_cat, y_tr, y_va, opt\n",
    "        if scheduler is not None:\n",
    "            del scheduler\n",
    "        gc.collect()\n",
    "\n",
    "    return best_losses, best_predictions, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "print(\"{}{: >25}{}{: >10}{}\".format('|','Variable Name','|','Memory','|'))\n",
    "print(\" ------------------------------------ \")\n",
    "for var_name in dir():\n",
    "    if not var_name.startswith(\"_\") and sys.getsizeof(eval(var_name)) > 10000: #ここだけアレンジ\n",
    "        print(\"{}{: >25}{}{: >10}{}\".format('|',var_name,'|',sys.getsizeof(eval(var_name)),'|'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "del df, df_train\n",
    "gc.collect()\n",
    "\n",
    "if PREDICT_MLP:\n",
    "    model_paths = []\n",
    "    \n",
    "    if SHORTCUT_NN_IN_1ST_STAGE and IS_1ST_STAGE:\n",
    "        epochs = 3\n",
    "        valid_th = 100\n",
    "    else:\n",
    "        epochs = 30\n",
    "        valid_th = NN_VALID_TH\n",
    "    \n",
    "    for i in range(NN_NUM_MODELS):\n",
    "        # MLP\n",
    "        nn_losses, nn_preds, scaler = train_nn(X, y, \n",
    "                                                 [folds[-1]], \n",
    "                                                 device=device, \n",
    "                                                 batch_size=512,\n",
    "                                                 mlp_bn=True,\n",
    "                                                 mlp_hidden=256,\n",
    "                                                 mlp_dropout=0.0,\n",
    "                                                 emb_dim=30,\n",
    "                                                 epochs=epochs,\n",
    "                                                 lr=0.002,\n",
    "                                                 max_lr=0.0055,\n",
    "                                                 weight_decay=1e-7,\n",
    "                                                model_path='mlp_fold_{}' + f\"_seed{i}.pth\",\n",
    "                                                seed=i)\n",
    "\n",
    "        print(nn_losses)\n",
    "        if nn_losses[0] < NN_VALID_TH:\n",
    "            print(f'model of seed {i} added.')\n",
    "            model_paths.append(f'artifacts/mlp_fold_0_seed{i}.pth')\n",
    "            np.save(f'fold3_pred_mlp_seed{i}.npy', nn_preds[0])\n",
    "\n",
    "    mlp_model = [torch.load(path, device) for path in model_paths]\n",
    "    \n",
    "    print(f'total {len(mlp_model)} models will be used.')\n",
    "if PREDICT_CNN:\n",
    "    model_paths = []\n",
    "        \n",
    "    if SHORTCUT_NN_IN_1ST_STAGE and IS_1ST_STAGE:\n",
    "        epochs = 3\n",
    "        valid_th = 100\n",
    "    else:\n",
    "        epochs = 50\n",
    "        valid_th = NN_VALID_TH\n",
    "\n",
    "    for i in range(NN_NUM_MODELS):\n",
    "        nn_losses, nn_preds, scaler = train_nn(X, y, \n",
    "                                               [folds[-1]], \n",
    "                                               device=device, \n",
    "                                               cnn_hidden=8*128,\n",
    "                                                  batch_size=1280,\n",
    "                                                  model_type='cnn',\n",
    "                                                  emb_dim=30,\n",
    "                                                  epochs=epochs, #epochs,\n",
    "                                                  cnn_channel1=128,\n",
    "                                                  cnn_channel2=3*128,\n",
    "                                                  cnn_channel3=3*128,\n",
    "                                                  lr=0.00038, #0.0011,\n",
    "                                                  #scaler_type=scaler_type,\n",
    "                                                  max_lr=0.0013,\n",
    "                                                  weight_decay=6.5e-6,\n",
    "                                                optimizer_type='adam',\n",
    "                                               scheduler_type='reduce',\n",
    "                                                model_path='cnn_fold_{}' + f\"_seed{i}.pth\",\n",
    "                                                seed=i,\n",
    "                                              cnn_dropout=0.0,\n",
    "                                              cnn_weight_norm=True,\n",
    "                                              cnn_leaky_relu=False,\n",
    "                                              patience=8,\n",
    "                                              factor=0.3)\n",
    "        if nn_losses[0] < valid_th:\n",
    "            model_paths.append(f'artifacts/cnn_fold_0_seed{i}.pth')\n",
    "            np.save(f'fold3_pred_cnn_seed{i}.npy', nn_preds[0])\n",
    "    cnn_model = [torch.load(path, device) for path in model_paths]\n",
    "        \n",
    "    print(f'total {len(cnn_model)} models will be used.')\n",
    "    \n",
    "if PREDICT_TABNET:\n",
    "    tab_model = []\n",
    "        \n",
    "    if SHORTCUT_NN_IN_1ST_STAGE and IS_1ST_STAGE:\n",
    "        epochs = 10\n",
    "        valid_th = 100\n",
    "    else:\n",
    "        epochs = 250\n",
    "        valid_th = NN_VALID_TH\n",
    "\n",
    "    for i in range(TABNET_NUM_MODELS):\n",
    "        nn_losses, nn_preds, scaler, model = train_tabnet(X, y,  \n",
    "                                                          [folds[-1]], \n",
    "                                                          batch_size=512,\n",
    "                                                          epochs=250, #epochs,\n",
    "                                                          lr=0.02,\n",
    "                                                          patience=50,\n",
    "                                                          factor=0.5,\n",
    "                                                          gamma=1.5,\n",
    "                                                          lambda_sparse=3e-7,\n",
    "                                                          seed=i)\n",
    "        if nn_losses[0] < valid_th:\n",
    "            tab_model.append(model)\n",
    "            np.save(f'fold3_pred_tab_seed{i}.npy', nn_preds[0])\n",
    "\n",
    "    print(f'total {len(tab_model)} models will be used.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
